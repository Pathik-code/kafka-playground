{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kafka Consumer Basics\n",
    "\n",
    "This notebook demonstrates how to consume messages from Kafka topics.\n",
    "\n",
    "## Topics Covered:\n",
    "- Creating consumers\n",
    "- Consumer groups\n",
    "- Manual and automatic offset management\n",
    "- Consuming from specific partitions\n",
    "- Seeking to specific offsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from kafka import KafkaConsumer, TopicPartition\n",
    "from kafka.errors import KafkaError\n",
    "\n",
    "# Kafka cluster connection\n",
    "KAFKA_SERVERS = os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'kafka1:29092,kafka2:29093,kafka3:29094')\n",
    "print(f\"Connecting to Kafka at: {KAFKA_SERVERS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a Simple Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_name = 'test-topic'\n",
    "\n",
    "# Create consumer\n",
    "consumer = KafkaConsumer(\n",
    "    topic_name,\n",
    "    bootstrap_servers=KAFKA_SERVERS.split(','),\n",
    "    value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "    key_deserializer=lambda k: k.decode('utf-8') if k else None,\n",
    "    group_id='test-consumer-group',\n",
    "    auto_offset_reset='earliest',  # Start from beginning if no offset exists\n",
    "    enable_auto_commit=True,\n",
    "    auto_commit_interval_ms=1000\n",
    ")\n",
    "\n",
    "print(\"✓ Consumer created successfully!\")\n",
    "print(f\"  Subscribed to: {consumer.subscription()}\")\n",
    "print(f\"  Consumer group: test-consumer-group\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Consume Messages (with timeout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consume messages for 10 seconds\n",
    "import time\n",
    "\n",
    "message_count = 0\n",
    "start_time = time.time()\n",
    "timeout = 10  # seconds\n",
    "\n",
    "print(f\"Consuming messages for {timeout} seconds...\\n\")\n",
    "\n",
    "try:\n",
    "    for message in consumer:\n",
    "        message_count += 1\n",
    "        print(f\"Message {message_count}:\")\n",
    "        print(f\"  Key: {message.key}\")\n",
    "        print(f\"  Value: {message.value}\")\n",
    "        print(f\"  Partition: {message.partition}\")\n",
    "        print(f\"  Offset: {message.offset}\")\n",
    "        print(f\"  Timestamp: {message.timestamp}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Break after timeout\n",
    "        if time.time() - start_time > timeout:\n",
    "            break\n",
    "            \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nConsumer interrupted by user\")\n",
    "\n",
    "print(f\"\\n✓ Consumed {message_count} messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Consume Fixed Number of Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new consumer to start fresh\n",
    "consumer2 = KafkaConsumer(\n",
    "    topic_name,\n",
    "    bootstrap_servers=KAFKA_SERVERS.split(','),\n",
    "    value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "    group_id='test-consumer-group-2',\n",
    "    auto_offset_reset='earliest'\n",
    ")\n",
    "\n",
    "# Consume exactly 5 messages\n",
    "messages_to_consume = 5\n",
    "consumed = []\n",
    "\n",
    "print(f\"Consuming {messages_to_consume} messages...\\n\")\n",
    "\n",
    "for i, message in enumerate(consumer2):\n",
    "    consumed.append(message.value)\n",
    "    print(f\"{i+1}. {message.value}\")\n",
    "    \n",
    "    if i + 1 >= messages_to_consume:\n",
    "        break\n",
    "\n",
    "consumer2.close()\n",
    "print(f\"\\n✓ Consumed {len(consumed)} messages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Manual Offset Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consumer with manual offset commit\n",
    "manual_consumer = KafkaConsumer(\n",
    "    topic_name,\n",
    "    bootstrap_servers=KAFKA_SERVERS.split(','),\n",
    "    value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "    group_id='manual-offset-group',\n",
    "    enable_auto_commit=False,  # Manual commit\n",
    "    auto_offset_reset='earliest'\n",
    ")\n",
    "\n",
    "message_count = 0\n",
    "batch_size = 3\n",
    "\n",
    "print(f\"Manually committing offsets every {batch_size} messages...\\n\")\n",
    "\n",
    "for message in manual_consumer:\n",
    "    message_count += 1\n",
    "    print(f\"Consumed: {message.value}\")\n",
    "    \n",
    "    # Commit offset after processing every batch_size messages\n",
    "    if message_count % batch_size == 0:\n",
    "        manual_consumer.commit()\n",
    "        print(f\"  ✓ Committed offset after {message_count} messages\\n\")\n",
    "    \n",
    "    if message_count >= 9:\n",
    "        break\n",
    "\n",
    "# Final commit\n",
    "manual_consumer.commit()\n",
    "manual_consumer.close()\n",
    "print(f\"\\n✓ Processed {message_count} messages with manual commits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Consume from Specific Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create consumer for specific partition\n",
    "partition_consumer = KafkaConsumer(\n",
    "    bootstrap_servers=KAFKA_SERVERS.split(','),\n",
    "    value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "    auto_offset_reset='earliest'\n",
    ")\n",
    "\n",
    "# Assign specific partition\n",
    "partition = TopicPartition(topic_name, 0)\n",
    "partition_consumer.assign([partition])\n",
    "\n",
    "print(f\"Consuming from {topic_name}, partition 0...\\n\")\n",
    "\n",
    "count = 0\n",
    "for message in partition_consumer:\n",
    "    count += 1\n",
    "    print(f\"{count}. Partition {message.partition}, Offset {message.offset}: {message.value}\")\n",
    "    \n",
    "    if count >= 5:\n",
    "        break\n",
    "\n",
    "partition_consumer.close()\n",
    "print(f\"\\n✓ Consumed {count} messages from partition 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Seek to Specific Offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create consumer\n",
    "seek_consumer = KafkaConsumer(\n",
    "    topic_name,\n",
    "    bootstrap_servers=KAFKA_SERVERS.split(','),\n",
    "    value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "    group_id='seek-consumer-group'\n",
    ")\n",
    "\n",
    "# Wait for partition assignment\n",
    "partitions = seek_consumer.assignment()\n",
    "while not partitions:\n",
    "    seek_consumer.poll(timeout_ms=100)\n",
    "    partitions = seek_consumer.assignment()\n",
    "\n",
    "# Seek to offset 5 on first partition\n",
    "if partitions:\n",
    "    partition = list(partitions)[0]\n",
    "    seek_consumer.seek(partition, 5)\n",
    "    print(f\"Seeking to offset 5 on partition {partition.partition}...\\n\")\n",
    "    \n",
    "    count = 0\n",
    "    for message in seek_consumer:\n",
    "        count += 1\n",
    "        print(f\"Offset {message.offset}: {message.value}\")\n",
    "        \n",
    "        if count >= 3:\n",
    "            break\n",
    "\n",
    "seek_consumer.close()\n",
    "print(f\"\\n✓ Consumed {count} messages starting from offset 5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Get Partition Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create consumer to inspect partitions\n",
    "info_consumer = KafkaConsumer(\n",
    "    topic_name,\n",
    "    bootstrap_servers=KAFKA_SERVERS.split(','),\n",
    "    group_id='info-group'\n",
    ")\n",
    "\n",
    "# Get partition info\n",
    "partitions = info_consumer.partitions_for_topic(topic_name)\n",
    "print(f\"Topic '{topic_name}' has {len(partitions)} partitions: {partitions}\")\n",
    "\n",
    "# Get beginning and end offsets\n",
    "partition_list = [TopicPartition(topic_name, p) for p in partitions]\n",
    "beginning_offsets = info_consumer.beginning_offsets(partition_list)\n",
    "end_offsets = info_consumer.end_offsets(partition_list)\n",
    "\n",
    "print(\"\\nPartition Details:\")\n",
    "for partition in partition_list:\n",
    "    begin = beginning_offsets[partition]\n",
    "    end = end_offsets[partition]\n",
    "    messages = end - begin\n",
    "    print(f\"  Partition {partition.partition}: {messages} messages (offset {begin} to {end})\")\n",
    "\n",
    "info_consumer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close all consumers\n",
    "consumer.close()\n",
    "print(\"✓ All consumers closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Consumer Groups**: Multiple consumers in same group share partition load\n",
    "2. **auto_offset_reset**: 'earliest' starts from beginning, 'latest' starts from end\n",
    "3. **Manual Commits**: Better control over exactly when offsets are committed\n",
    "4. **Partition Assignment**: Can consume from specific partitions\n",
    "5. **Seeking**: Can jump to specific offsets for replay or skipping\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Try the Admin Operations notebook (03_kafka_admin_operations.ipynb) to manage topics!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
