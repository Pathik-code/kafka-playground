{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Stream Processing with Kafka\n",
                "\n",
                "This notebook demonstrates real-time stream processing patterns using Kafka.\n",
                "\n",
                "## Topics Covered:\n",
                "- Real-time data aggregation\n",
                "- Windowing operations\n",
                "- Stateful processing\n",
                "- Stream transformations\n",
                "- Pattern detection"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "import time\n",
                "from datetime import datetime, timedelta\n",
                "from collections import defaultdict, deque\n",
                "from kafka import KafkaProducer, KafkaConsumer\n",
                "import threading\n",
                "\n",
                "# Kafka cluster connection\n",
                "KAFKA_SERVERS = os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'kafka1:29092,kafka2:29093,kafka3:29094')\n",
                "print(f\"Connecting to Kafka at: {KAFKA_SERVERS}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Setup Topics for Streaming"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from kafka.admin import KafkaAdminClient, NewTopic\n",
                "from kafka.errors import TopicAlreadyExistsError\n",
                "\n",
                "admin_client = KafkaAdminClient(bootstrap_servers=KAFKA_SERVERS.split(','))\n",
                "\n",
                "topics = [\n",
                "    NewTopic(name='sensor-data', num_partitions=3, replication_factor=2),\n",
                "    NewTopic(name='alerts', num_partitions=2, replication_factor=2)\n",
                "]\n",
                "\n",
                "try:\n",
                "    admin_client.create_topics(new_topics=topics, validate_only=False)\n",
                "    print(\"âœ“ Topics created: sensor-data, alerts\")\n",
                "except TopicAlreadyExistsError:\n",
                "    print(\"âœ“ Topics already exist\")\n",
                "\n",
                "admin_client.close()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Generate Simulated Sensor Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import random\n",
                "\n",
                "def generate_sensor_data(producer, duration_seconds=30):\n",
                "    \"\"\"Generate simulated sensor readings\"\"\"\n",
                "    sensor_ids = ['sensor-1', 'sensor-2', 'sensor-3', 'sensor-4']\n",
                "    start_time = time.time()\n",
                "    count = 0\n",
                "    \n",
                "    while time.time() - start_time < duration_seconds:\n",
                "        sensor_id = random.choice(sensor_ids)\n",
                "        \n",
                "        # Generate data with occasional anomalies\n",
                "        temperature = random.uniform(20, 30)\n",
                "        if random.random() < 0.1:  # 10% chance of anomaly\n",
                "            temperature = random.uniform(50, 70)\n",
                "        \n",
                "        data = {\n",
                "            'sensor_id': sensor_id,\n",
                "            'timestamp': datetime.now().isoformat(),\n",
                "            'temperature': round(temperature, 2),\n",
                "            'humidity': round(random.uniform(40, 80), 2),\n",
                "            'pressure': round(random.uniform(1010, 1020), 2)\n",
                "        }\n",
                "        \n",
                "        producer.send('sensor-data', value=data, key=sensor_id)\n",
                "        count += 1\n",
                "        time.sleep(0.5)  # Send every 500ms\n",
                "    \n",
                "    producer.flush()\n",
                "    print(f\"\\nâœ“ Generated {count} sensor readings\")\n",
                "\n",
                "# Create producer\n",
                "producer = KafkaProducer(\n",
                "    bootstrap_servers=KAFKA_SERVERS.split(','),\n",
                "    value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
                "    key_serializer=lambda k: k.encode('utf-8')\n",
                ")\n",
                "\n",
                "print(\"Starting data generation for 30 seconds...\")\n",
                "print(\"(This will run in background while we process)\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Real-Time Aggregation (Windowed Average)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class WindowedAggregator:\n",
                "    \"\"\"Calculate moving average over time window\"\"\"\n",
                "    \n",
                "    def __init__(self, window_seconds=10):\n",
                "        self.window_seconds = window_seconds\n",
                "        self.data = defaultdict(deque)\n",
                "    \n",
                "    def add(self, sensor_id, value, timestamp):\n",
                "        self.data[sensor_id].append((timestamp, value))\n",
                "        self._cleanup(sensor_id, timestamp)\n",
                "    \n",
                "    def _cleanup(self, sensor_id, current_time):\n",
                "        cutoff = current_time - timedelta(seconds=self.window_seconds)\n",
                "        while self.data[sensor_id] and self.data[sensor_id][0][0] < cutoff:\n",
                "            self.data[sensor_id].popleft()\n",
                "    \n",
                "    def get_average(self, sensor_id):\n",
                "        if not self.data[sensor_id]:\n",
                "            return None\n",
                "        values = [v for _, v in self.data[sensor_id]]\n",
                "        return sum(values) / len(values)\n",
                "    \n",
                "    def get_count(self, sensor_id):\n",
                "        return len(self.data[sensor_id])\n",
                "\n",
                "# Start data generation in background\n",
                "generator_thread = threading.Thread(target=generate_sensor_data, args=(producer, 30))\n",
                "generator_thread.daemon = True\n",
                "generator_thread.start()\n",
                "\n",
                "# Create consumer for stream processing\n",
                "consumer = KafkaConsumer(\n",
                "    'sensor-data',\n",
                "    bootstrap_servers=KAFKA_SERVERS.split(','),\n",
                "    value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
                "    auto_offset_reset='latest',\n",
                "    group_id='stream-processor'\n",
                ")\n",
                "\n",
                "# Process stream with windowed aggregation\n",
                "aggregator = WindowedAggregator(window_seconds=10)\n",
                "start_time = time.time()\n",
                "process_duration = 25\n",
                "\n",
                "print(\"Processing sensor data stream...\\n\")\n",
                "\n",
                "for message in consumer:\n",
                "    data = message.value\n",
                "    sensor_id = data['sensor_id']\n",
                "    temperature = data['temperature']\n",
                "    timestamp = datetime.fromisoformat(data['timestamp'])\n",
                "    \n",
                "    # Add to window\n",
                "    aggregator.add(sensor_id, temperature, timestamp)\n",
                "    \n",
                "    # Calculate moving average\n",
                "    avg_temp = aggregator.get_average(sensor_id)\n",
                "    count = aggregator.get_count(sensor_id)\n",
                "    \n",
                "    print(f\"{sensor_id}: Current={temperature:.1f}Â°C, 10s Avg={avg_temp:.1f}Â°C (n={count})\")\n",
                "    \n",
                "    # Stop after duration\n",
                "    if time.time() - start_time > process_duration:\n",
                "        break\n",
                "\n",
                "consumer.close()\n",
                "print(\"\\nâœ“ Stream processing completed\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Anomaly Detection (Pattern Recognition)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class AnomalyDetector:\n",
                "    \"\"\"Detect temperature anomalies\"\"\"\n",
                "    \n",
                "    def __init__(self, threshold=40.0):\n",
                "        self.threshold = threshold\n",
                "        self.alert_producer = KafkaProducer(\n",
                "            bootstrap_servers=KAFKA_SERVERS.split(','),\n",
                "            value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
                "        )\n",
                "    \n",
                "    def check(self, data):\n",
                "        if data['temperature'] > self.threshold:\n",
                "            alert = {\n",
                "                'alert_type': 'HIGH_TEMPERATURE',\n",
                "                'sensor_id': data['sensor_id'],\n",
                "                'temperature': data['temperature'],\n",
                "                'threshold': self.threshold,\n",
                "                'timestamp': datetime.now().isoformat(),\n",
                "                'severity': 'WARNING' if data['temperature'] < 60 else 'CRITICAL'\n",
                "            }\n",
                "            self.alert_producer.send('alerts', value=alert)\n",
                "            return alert\n",
                "        return None\n",
                "    \n",
                "    def close(self):\n",
                "        self.alert_producer.close()\n",
                "\n",
                "# Start fresh data generation\n",
                "generator_thread2 = threading.Thread(target=generate_sensor_data, args=(producer, 20))\n",
                "generator_thread2.daemon = True\n",
                "generator_thread2.start()\n",
                "\n",
                "time.sleep(1)  # Wait for generator to start\n",
                "\n",
                "# Create consumer and detector\n",
                "consumer2 = KafkaConsumer(\n",
                "    'sensor-data',\n",
                "    bootstrap_servers=KAFKA_SERVERS.split(','),\n",
                "    value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
                "    auto_offset_reset='latest',\n",
                "    group_id='anomaly-detector'\n",
                ")\n",
                "\n",
                "detector = AnomalyDetector(threshold=40.0)\n",
                "\n",
                "print(\"Detecting anomalies (temperature > 40Â°C)...\\n\")\n",
                "\n",
                "start_time = time.time()\n",
                "alert_count = 0\n",
                "\n",
                "for message in consumer2:\n",
                "    data = message.value\n",
                "    alert = detector.check(data)\n",
                "    \n",
                "    if alert:\n",
                "        alert_count += 1\n",
                "        print(f\"ðŸš¨ ALERT: {alert['sensor_id']} - {alert['temperature']}Â°C ({alert['severity']})\")\n",
                "    else:\n",
                "        print(f\"âœ“ {data['sensor_id']}: {data['temperature']}Â°C - Normal\")\n",
                "    \n",
                "    if time.time() - start_time > 15:\n",
                "        break\n",
                "\n",
                "detector.close()\n",
                "consumer2.close()\n",
                "\n",
                "print(f\"\\nâœ“ Detected {alert_count} anomalies\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Stream Enrichment (Join Pattern)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sensor metadata (simulated lookup table)\n",
                "sensor_metadata = {\n",
                "    'sensor-1': {'location': 'Building A', 'floor': 1, 'room': 'Server Room'},\n",
                "    'sensor-2': {'location': 'Building A', 'floor': 2, 'room': 'Office 201'},\n",
                "    'sensor-3': {'location': 'Building B', 'floor': 1, 'room': 'Lab'},\n",
                "    'sensor-4': {'location': 'Building B', 'floor': 3, 'room': 'Storage'}\n",
                "}\n",
                "\n",
                "def enrich_sensor_data(data):\n",
                "    \"\"\"Enrich sensor data with metadata\"\"\"\n",
                "    sensor_id = data['sensor_id']\n",
                "    metadata = sensor_metadata.get(sensor_id, {})\n",
                "    return {**data, **metadata}\n",
                "\n",
                "# Consume and enrich\n",
                "consumer3 = KafkaConsumer(\n",
                "    'sensor-data',\n",
                "    bootstrap_servers=KAFKA_SERVERS.split(','),\n",
                "    value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
                "    auto_offset_reset='earliest',\n",
                "    group_id='enrichment-processor'\n",
                ")\n",
                "\n",
                "print(\"Enriching sensor data with metadata...\\n\")\n",
                "\n",
                "count = 0\n",
                "for message in consumer3:\n",
                "    enriched = enrich_sensor_data(message.value)\n",
                "    print(f\"ðŸ“ {enriched['sensor_id']} @ {enriched.get('location', 'Unknown')} \")\n",
                "    print(f\"   {enriched.get('room', 'N/A')} - Temp: {enriched['temperature']}Â°C\")\n",
                "    print()\n",
                "    \n",
                "    count += 1\n",
                "    if count >= 5:\n",
                "        break\n",
                "\n",
                "consumer3.close()\n",
                "print(\"âœ“ Enrichment completed\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Stateful Processing (Session Tracking)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SessionTracker:\n",
                "    \"\"\"Track sensor sessions and calculate statistics\"\"\"\n",
                "    \n",
                "    def __init__(self):\n",
                "        self.sessions = defaultdict(lambda: {\n",
                "            'count': 0,\n",
                "            'total_temp': 0,\n",
                "            'min_temp': float('inf'),\n",
                "            'max_temp': float('-inf'),\n",
                "            'first_seen': None,\n",
                "            'last_seen': None\n",
                "        })\n",
                "    \n",
                "    def update(self, sensor_id, data):\n",
                "        session = self.sessions[sensor_id]\n",
                "        temp = data['temperature']\n",
                "        timestamp = data['timestamp']\n",
                "        \n",
                "        session['count'] += 1\n",
                "        session['total_temp'] += temp\n",
                "        session['min_temp'] = min(session['min_temp'], temp)\n",
                "        session['max_temp'] = max(session['max_temp'], temp)\n",
                "        session['last_seen'] = timestamp\n",
                "        \n",
                "        if session['first_seen'] is None:\n",
                "            session['first_seen'] = timestamp\n",
                "    \n",
                "    def get_stats(self, sensor_id):\n",
                "        session = self.sessions[sensor_id]\n",
                "        if session['count'] == 0:\n",
                "            return None\n",
                "        \n",
                "        return {\n",
                "            'sensor_id': sensor_id,\n",
                "            'readings': session['count'],\n",
                "            'avg_temp': session['total_temp'] / session['count'],\n",
                "            'min_temp': session['min_temp'],\n",
                "            'max_temp': session['max_temp'],\n",
                "            'duration': session['last_seen']\n",
                "        }\n",
                "\n",
                "tracker = SessionTracker()\n",
                "\n",
                "consumer4 = KafkaConsumer(\n",
                "    'sensor-data',\n",
                "    bootstrap_servers=KAFKA_SERVERS.split(','),\n",
                "    value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
                "    auto_offset_reset='earliest',\n",
                "    group_id='session-tracker'\n",
                ")\n",
                "\n",
                "print(\"Tracking sensor sessions...\\n\")\n",
                "\n",
                "count = 0\n",
                "for message in consumer4:\n",
                "    data = message.value\n",
                "    tracker.update(data['sensor_id'], data)\n",
                "    count += 1\n",
                "    \n",
                "    if count >= 20:\n",
                "        break\n",
                "\n",
                "consumer4.close()\n",
                "\n",
                "# Print session statistics\n",
                "print(\"\\nSession Statistics:\\n\")\n",
                "for sensor_id in sorted(tracker.sessions.keys()):\n",
                "    stats = tracker.get_stats(sensor_id)\n",
                "    if stats:\n",
                "        print(f\"{stats['sensor_id']}:\")\n",
                "        print(f\"  Readings: {stats['readings']}\")\n",
                "        print(f\"  Avg Temp: {stats['avg_temp']:.1f}Â°C\")\n",
                "        print(f\"  Range: {stats['min_temp']:.1f}Â°C - {stats['max_temp']:.1f}Â°C\")\n",
                "        print()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Cleanup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Close producer\n",
                "producer.close()\n",
                "print(\"âœ“ All resources cleaned up\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Key Takeaways\n",
                "\n",
                "1. **Windowing**: Use time-based or count-based windows for aggregations\n",
                "2. **Stateful Processing**: Maintain state across messages for complex logic\n",
                "3. **Stream Enrichment**: Join streaming data with reference data\n",
                "4. **Pattern Detection**: Implement real-time anomaly detection\n",
                "5. **Multiple Consumers**: Different consumers can process same stream differently\n",
                "\n",
                "## Stream Processing Patterns\n",
                "\n",
                "- **Filter**: Select specific messages based on criteria\n",
                "- **Map**: Transform each message\n",
                "- **Aggregate**: Combine multiple messages (windowed)\n",
                "- **Join**: Combine data from multiple streams\n",
                "- **Branch**: Route messages to different processors\n",
                "\n",
                "## Production Considerations\n",
                "\n",
                "- Use appropriate window sizes for your use case\n",
                "- Implement checkpointing for fault tolerance\n",
                "- Consider using Kafka Streams or Flink for complex processing\n",
                "- Monitor consumer lag and processing latency"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}